# -*- coding: utf-8 -*-
"""tagger.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CZzxoldkESlMdqNGu3v0nYvEATlgKa45
"""

!pip install sklearn_crfsuite
!pip install fasttext

from nltk.tokenize import TreebankWordTokenizer as twt
from nltk.tokenize import PunktSentenceTokenizer as pkt
from nltk import pos_tag
import glob
import sklearn
import sklearn_crfsuite
import fasttext
from google.colab import drive
drive.mount('/gdrive')

drive.mount('/gdrive')
SPAN_DIR = 'gdrive/My Drive/MS_Project/MS_Final_Project/datasets/'
 = '/directoryForArticleSpans'
ARTICLE_DIR = '/train-article/task1'
FASTTEXT_MODEL = '/train-article/task1'

class Tagger(self):
    '''The Tagger class is responsible for all of
    the annotation/tagging needed to create features
    for the sequence model. 

    The output of this class is a single pickle file 
    for whatever article was passed to it. The file
    consists of N-D arrays where each row is a sentence
    and every index is a token json object whose key:value 
    pairs are the features and the propaganda tag.
    The latter of course being the target value (y)'''
    def __init__(self):

        # Models
        self.token_tagger = None  # Currently twt
        self.pos_tagger = None 
        self.sent_tagger = None  # Currently punkt
        self.fasttext = None
        
        # Propaganda Span Dictionary: {'articlename':[(start, fin)]}
        self.span_dict = {}

        # Unadultered article text
        self.raw_text = '' 
        
        # Article name
        self.article_name = '' 

        # List of token positions (start, end) for each token
        self.token_spans = [] 

        # List of sentence positions (start, end) for each sentence
        self.sent_spans = []

        # List of lists of spans corresponding to sentences
        self.unflat_sent = []

        # POS tags for correspoding tokens
        self.pos_tags = [] 
        
        # TRI-GRAM Arrays: [(prev_tok, next_tok)]
        self.trigrams = []

        
    def clear(self):
        '''Clears all article specific variables
        Does not delete any of the models or the propaganda
        span dictionary'''
        
        self.raw_text = ''
        self.article_name = ''
        self.token_spans = []
        self.sent_spans = []
        self.unflat_sent = []
        self.pos_tags = []                                                                                                                                                                    
        self.trigrams = []
        
        
    def build_span_dict(self, span_directory):
        ''' This function reads in all the spans from all of the task1 directory
        This can be dumped into a pickle file using pickle_dump()
        One this dictionary has been created it can be reloaded using
        load_span_dict()
        '''

        files = glob(span_directory, '.*.labels')
        for article in files:
            if not os.stat(article).st_size == 0:
                spans = list(open(article, "r"))
                spans = [line.split('\t') for line in spans ]
                name = spans[0][0]
                self.span_dict[name] = [(span[1], span[2]) for span in spans] 
        
    def pickle_dump(self, object, fout):
        pickle.dump(object, open(fout, "wb"))
    
    def load_span_dict(self, span_dict_pickle):
        '''This function reads in the already created span dick
        It expects a pickle file
        '''
        self.span_dict = pickle.load(open(span_dict_pickle))


    def load_article(self, article_fin):
        '''This function reads in an article and initializes a string
        object to represent the text.
        
        Creates: self.raw_text
        Creates: self.article_name
        '''
        return
    
    def create_token_spans(self):
        '''This function creates a list of token spans from raw_text. The token
        indices are respective of character indices in the text.

        Creates: self.token_spans
        
        '''
        if self.tokenizer is None:
            self.tokenizer = twt()
        self.token_spans = list(self.tokenizer.span_tokenize(self.raw_text))

    
    def create_sent_spans(self):
        '''This function creates a list of sentence spans from raw_text.
        Sentence boundaries are needed because CRFs/Sequence Models require
        sequences
        
        Creates: self.sent_spans
        '''
        if self.sent_tagger is None:
            self.sent_tagger = pkt()
        self.sent_spans = list(self.sent_tagger.span_tokenize(self.raw_text))

    def create_sent_lists(self):
        '''Creates an array object where each row is a list of numbers 
        corresponding to the tokens in each sentence. This is needed for both 
        CRF/Sequence taggers and the pos_tagger which takes a list of tokens as 
        its input. This function is not looking at the char level indeces of 
        raw_text but instead uses the indices in token_spans

        Creates: self.unflat_sent
        '''
        sent_tok_index = [[] for sent in self.sent_spans]
        i = 0
        for st, fin in self.token_spans:
            j = 0
            for s_st, s_f in self.sent_spans:
                sent_range = set(range(s_st, s_f))
                if st in sent_range:
                    sent_tok_index[j].append(i)
                j = j + 1
            i = i + 1
        self.unflat_sent = sent_tok_index``
    
    def pos_tag(self):
        '''Tags tokens using indices from token_spans and sent_spans 
        to get strings from text_raw

        Creates: self.pos_tags
        '''
        if self.pos_tagger is None:
            self.pos_tagger = pos_tag()
        
            
        

    def trigram(self):
        '''Uses tself.unflat_sent he token tag indices to 
        get the previous and next tokens 
        '''
        return

from google.colab import drive
drive.mount('/content/gdrive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /My Drive/MS_Project/MS_Final_Project/datasets/

cd /root/